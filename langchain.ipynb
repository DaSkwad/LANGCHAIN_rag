{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f4459b",
   "metadata": {},
   "source": [
    "![LangChain](img/langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511a0f0",
   "metadata": {},
   "source": [
    "Les systèmes **RAG (Retrieval-Augmented Generation)** dans LangChain permettent aux modèles de langage de s’appuyer sur des **connaissances externes** pour produire des réponses plus précises, actualisées et pertinentes.\n",
    "\n",
    "Contrairement à un simple LLM qui génère une réponse uniquement à partir de ce qu’il a appris pendant son entraînement, un système RAG interroge une base de documents pour retrouver des morceaux d’information pertinents – appelés **chunks** – et les injecte dans le prompt du LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb160db",
   "metadata": {},
   "source": [
    "![RAG](img/rag.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3075ae",
   "metadata": {},
   "source": [
    "**Que montre le schéma ci-dessus ?**\n",
    "\n",
    "Le processus se divise en **deux grandes phases** : **préparation des documents** et **traitement des requêtes**.\n",
    "\n",
    "**Préparation des documents (à gauche)**\n",
    "- (1) Un fichier (document source) est divisé en **chunks**, c’est-à-dire en petits segments de texte.\n",
    "- (2) Chaque chunk est passé dans un LLM Embedder, un encodeur qui transforme le texte en un vecteur numérique (**embeddings**).\n",
    "- (3) Ces vecteurs sont ensuite stockés dans un Vector Store, une base de données spécialisée pour les recherches par **similarité sémantique**.\n",
    "\n",
    "**Traitement des requêtes (à droite)**\n",
    "- (a) Lorsqu’un utilisateur emet une requête, celle-ci est à son tour encodée via **le même LLM Embedder** pour obtenir son vecteur.\n",
    "- (b) Ce vecteur est utilisé par le **Retriever**, qui compare la requête aux vecteurs des **chunks** pour trouver les plus similaires.\n",
    "- (c) Les chunks retrouvés sont envoyés au LLM, qui les utilise comme contexte pour formuler une réponse.\n",
    "\n",
    "\n",
    "En résumé, ce fonctionnement est illustré par la boucle :\n",
    "\n",
    "> Requête → Encodage → Recherche dans la base vectorielle → Récupération des chunks → Passage au LLM → Réponse contextuelle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7eb01a",
   "metadata": {},
   "source": [
    "# 1. Chargement du modèle LLM local\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb689",
   "metadata": {},
   "source": [
    "Dans cette section, nous chargeons un modèle de langage local grâce à **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion à une API externe.\n",
    "\n",
    "Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d’interagir facilement avec un modèle comme **llama3** ainsi qu'un **modèle d'embeddings** déjà téléchargés via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7301c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Chargement des clés d'API se trouvant dans le fichier .env.  \n",
    "# Ceci permet d'utiliser des modèles en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = ChatOllama(model=\"llama3\", temperature=0)\n",
    "#model = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"))\n",
    "\n",
    "# Modèle spécialisé pour convertir du texte en vecteurs (https://ollama.com/library/nomic-embed-text).\n",
    "# Il existe d'autres modèles d'embeddings (comme \"all-MiniLM-L6-v2\", \"text-embedding-ada-002\", etc.) \n",
    "# avec des performances et dimensions variées selon les cas d’usage (recherche sémantique, classification, etc.).\n",
    "embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a0cd4",
   "metadata": {},
   "source": [
    "# 2. RAG standard\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3369916",
   "metadata": {},
   "source": [
    "Le **RAG standard** consiste à :\n",
    "- formuler une requête explicite\n",
    "- interroger une base de documents vectorisée\n",
    "- utiliser un modèle LLM pour générer une réponse à partir des résultats retrouvés. \n",
    " \n",
    "Ce pipeline est **efficace pour des questions indépendantes, sans contexte conversationnel**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7de366",
   "metadata": {},
   "source": [
    "### 2.1 Préparation des documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699bd52",
   "metadata": {},
   "source": [
    "Nous initialisons les chemins nécessaires à la préparation des documents d’entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77475b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupère le chemin absolu du répertoire courant (là où le script est exécuté)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Nom du fichier texte contenant les comptes rendus de réunion\n",
    "file_name = \"meeting_reports.txt\"\n",
    "\n",
    "# Construit le chemin complet vers le fichier texte dans le dossier \"data\"\n",
    "file_path = os.path.join(current_dir, \"data\", file_name)\n",
    "\n",
    "# Définit le chemin du répertoire où sera stockée la base de données vectorielle (Chroma DB)\n",
    "db_dir = os.path.join(current_dir, \"data\", \"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed30a56",
   "metadata": {},
   "source": [
    "### 2.2 Initialisation du vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c526e6f",
   "metadata": {},
   "source": [
    "Nous vérifions ici si la base vectorielle existe déjà.  \n",
    "Si ce n’est pas le cas, le fichier source est chargé, découpé en morceaux, enrichi de métadonnées, puis indexé dans Chroma DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9884d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(db_dir):\n",
    "    print(\"Initializing vector store...\")\n",
    "\n",
    "    # Chargement du fichier texte brut contenant les documents\n",
    "    loader = TextLoader(file_path)\n",
    "    loaded_document = loader.load()\n",
    "\n",
    "    # Découpage du document en chunks de 1000 caractères avec un chevauchement de 0\n",
    "    # - chunk_size détermine la taille maximale de chaque morceau (en nombre de caractères ici : 1000)\n",
    "    # - chunk_overlap permet de conserver un chevauchement entre les morceaux pour éviter les coupures abruptes, ici il est à 0, donc sans recouvrement.\n",
    "    # - RecursiveCharacterTextSplitter est souvent préféré en pratique pour des documents textuels comme des comptes rendus, \n",
    "    #   des articles ou de la documentation technique, car il garde mieux le contexte sémantique.\n",
    "    #   Ce splitter tente d'abord de découper sur les sauts de ligne, puis sur les phrases, puis sur les mots, etc.\n",
    "    # ... d'autres Text Splitter comme CharachterTextSplitter existent. À approfondir si besoin\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(loaded_document)\n",
    "\n",
    "    # Ajout de métadonnées à chaque chunk (utile pour le filtrage ou le suivi de provenance).\n",
    "    # Ici 2 metadata sont ajoutés mais il pourrait en y avoir plus.\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"source\"] = file_path    # Chemin d'origine du document\n",
    "        chunk.metadata[\"category\"] = \"meeting\"  # Catégorie de contenu (à adapter selon les besoins)\n",
    "\n",
    "    # Création et persistance de la base vectorielle dans le dossier défini\n",
    "    db = Chroma.from_documents(chunks, embedder, persist_directory=db_dir)\n",
    "\n",
    "    print(\"Vector store created !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cbb986",
   "metadata": {},
   "source": [
    "### 2.3 Initialisation du moteur de recherche vectorielle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bc3f2",
   "metadata": {},
   "source": [
    "Une fois la base vectorielle Chroma initialisée avec les embeddings, nous la transformons en **moteur de recherche (retriever)**.  \n",
    "Cela permet de retrouver les documents les plus proches sémantiquement d’une question ou d’une requête.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee3ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/q0j81gmx5h7gvpg38k8hknzw0000gn/T/ipykernel_1472/573588974.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=db_dir, embedding_function=embedder)\n"
     ]
    }
   ],
   "source": [
    "# Chargement de la base vectorielle existante, avec liaison avec le même embedder ayant servi pour créer la base vectorielle\n",
    "db = Chroma(persist_directory=db_dir, embedding_function=embedder)\n",
    "\n",
    "# Conversion de la base Chroma en \"retriever\" pour effectuer des recherches par similarité\n",
    "# - search_type=\"similarity\" utilise la distance cosinus entre les vecteurs\n",
    "# - \"k\": 3 signifie que l'on souhaite récupérer les 3 documents les plus proches\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# 💡 Il est aussi possible d’utiliser d’autres types de recherche (search_type) :\n",
    "# - \"mmr\" (Maximal Marginal Relevance) : équilibre entre pertinence et diversité des résultats\n",
    "# - \"similarity_score_threshold\" : retourne uniquement les documents dont le score dépasse un certain seuil\n",
    "#      search_kwargs={\"score_threshold\": 0.8} permet par exemple de filtrer les résultats peu pertinents\n",
    "#\n",
    "# D’autres paramètres utiles dans search_kwargs :\n",
    "# - \"fetch_k\" : nombre de documents à récupérer avant le tri final (utile avec MMR)\n",
    "# - \"lambda_mult\" : pondération entre pertinence et diversité dans MMR\n",
    "# \n",
    "# Etc... à approfondir si besoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3318104",
   "metadata": {},
   "source": [
    "### 2.4 Exécution d’une requête de recherche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3186eb",
   "metadata": {},
   "source": [
    "Dans cette étape, nous combinons la recherche vectorielle avec un LLM.  \n",
    "L’objectif est de fournir une réponse pertinente à une question, en s’appuyant uniquement sur les documents retrouvés dans la base vectorielle.  \n",
    "Le modèle est guidé par un prompt structuré qui inclut la requête initiale et les contenus des chunks pertinents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0f69c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "En examinant les documents que vous m'avez fournis, je peux voir que les réunions concernant la société Neolink sont :\n",
       "\n",
       "* La réunion du 09/03/2025 (Bilan des incidents techniques du mois) : Cette réunion a porté sur les tâches réalisées et les éléments bloquants liés aux interactions avec NeoLink.\n",
       "* La réunion du 10/05/2025 (Réunion hebdomadaire de coordination technique) : Cette réunion a également abordé les enjeux liés à la communication interne, jugée parfois insuffisante, et aux besoins en ressources.\n",
       "\n",
       "Il n'y a pas d'autres réunions mentionnées dans les documents que vous m'avez fournis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Requête posée par l'utilisateur\n",
    "query = \"Quels sont les réunions concernant la société Neolink ?\"\n",
    "\n",
    "# Recherche des chunks vectoriellement proches de la question\n",
    "relevant_chunks = retriever.invoke(query)\n",
    "\n",
    "# Optionnel : affichage manuel des chunks retrouvés (utile pour debug ou vérification)\n",
    "# for i, chunk in enumerate(relevant_chunks, 1):\n",
    "#     print(f\"Chunk {i}:\\n{chunk.page_content}\\n\")\n",
    "\n",
    "# Construction du message d'entrée à envoyer au modèle\n",
    "# Nous incluons la question et le contenu des documents pour contraindre le LLM à ne répondre qu'en s'appuyant sur ces sources\n",
    "input_message = (\n",
    "    \"Voici des documents qui vont t'aider à répondre à la question : \"\n",
    "    + query\n",
    "    + \"\\n\\nDocuments pertinents : \\n\"\n",
    "    + \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "    + \"\\n\\nDonne une réponse basée uniquement sur les documents qui te sont fournis.\"\n",
    ")\n",
    "\n",
    "# Construction du message complet pour le LLM, avec un rôle système et un message utilisateur\n",
    "messages = [\n",
    "    SystemMessage(content=\"Tu es un assistant qui aide à retrouver tout type d'informations interne à une entreprise\"),\n",
    "    HumanMessage(content=input_message)\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c362eac",
   "metadata": {},
   "source": [
    "### 🧩 Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ccbc1",
   "metadata": {},
   "source": [
    "La société NovTech gère de nombreux documents internes :\n",
    "- des rapports d’incidents (panne, erreur technique, post-mortem),\n",
    "- des procédures opérationnelles (onboarding, accès système, déploiement…).\n",
    "\n",
    "Actuellement, les équipes perdent du temps à chercher les bonnes informations à travers des fichiers éparpillés.\n",
    "\n",
    "Votre objectif est de construire un assistant basé sur l'architecture RAG qui permettra :\n",
    "- de retrouver rapidement les procédures en cas de besoin,\n",
    "- de consulter les résolutions d’incidents similaires,\n",
    "- de répondre à des questions en langage naturel en s’appuyant uniquement sur les documents internes.\n",
    "\n",
    "Pour vous aider, vous pouvez suivre les étapes suivantes :\n",
    "1. Chargement des documents\n",
    "2. Découpage en chunks\n",
    "3. Indexation vectorielle\n",
    "4. Recherche contextuelle\n",
    "5. Génération de réponse\n",
    "\n",
    "ℹ️ Les documents de l'entreprise se trouve dans le dossier `data/novtech`.  \n",
    "💪🏻 **Bonus** : Rendre possible un filtrage par catégorie dans les recherches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49ae5cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a000884",
   "metadata": {},
   "source": [
    "# 3. RAG conversationnel\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843e3e1",
   "metadata": {},
   "source": [
    "Dans un cadre d’**interaction continue**, les utilisateurs posent souvent des questions implicites ou référentielles (ex. “Et lui ?”). Le **RAG conversationnel** ajoute une étape clé : la **reformulation de la question en prenant en compte l’historique du dialogue**.  \n",
    "\n",
    "Cette version de RAG permet de maintenir la pertinence des recherches dans la base vectorielle tout en conservant la fluidité de la conversation, ce qui la rend adaptée aux assistants IA ou aux chatbots avancés.\n",
    "\n",
    "**Exemple**\n",
    "\n",
    "Historique de la conversation :\n",
    "- Utilisateur : *Qui est le CEO de Tesla ?*\n",
    "- IA : *Elon Musk est le CEO de Tesla*.\n",
    "- Utilisateur : *Et de SpaceX ?*\n",
    "\n",
    "➡️ La question “Et de SpaceX ?” est ambiguë seule. Le moteur de recherche (retriever) ne sait pas de quoi il s’agit exactement.\n",
    "\n",
    "Avec une reformulation de la question de l'utilisateur cela donnerait : “Qui est le CEO de SpaceX ?”\n",
    "\n",
    "➡️ Résultat : la requête est claire, et la recherche dans la base vectorielle peut retourner les bons documents.\n",
    "\n",
    "**👍 LangChain facilite ce processus**\n",
    "\n",
    "LangChain fournit une abstraction prête à l’emploi grâce à la classe `ConversationalRetrievalChain`.\n",
    "Cette classe prend automatiquement en charge :\n",
    "- la reformulation de la question via le LLM\n",
    "- la recherche dans la base vectorielle\n",
    "- la génération de la réponse finale à partir des documents récupérés et de l’historique\n",
    "\n",
    "➡️ Elle encapsule ainsi toute la logique conversationnelle d’un RAG en une seule ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80b0361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Vous :** quit"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de la conversation.\n"
     ]
    }
   ],
   "source": [
    "# Chaîne RAG avec historique\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=db.as_retriever())\n",
    "\n",
    "# Boucle de chat\n",
    "chat_history = []\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    clear_output(wait=True)                         # Efface l'affichage précédent\n",
    "    display(Markdown(f\"**Vous :** {user_input}\"))   # Affiche la requête de l'utilisateur\n",
    "\n",
    "    if user_input.lower() in [\"stop\", \"exit\", \"quit\"]:\n",
    "        print(\"Fin de la conversation.\")\n",
    "        break\n",
    "\n",
    "    result = qa_chain({\"question\": user_input, \"chat_history\": chat_history})\n",
    "    display(Markdown(result[\"answer\"]))\n",
    "    chat_history.append((user_input, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a604a4",
   "metadata": {},
   "source": [
    "### 🧩 Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3873b2",
   "metadata": {},
   "source": [
    "Repartez de l'exercice précédent (NovTech), et implémentez un assistant de conversation continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
